{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- TSP Component ---\n",
    "def solve_tsp(distance_matrix):\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
    "    return col_ind, distance_matrix[row_ind, col_ind].sum()\n",
    "\n",
    "# --- ACO Component ---\n",
    "class AntColony:\n",
    "    def __init__(self, distance_matrix, n_ants, n_best, n_iterations, decay, alpha=1, beta=2):\n",
    "        self.distance_matrix = distance_matrix\n",
    "        self.pheromone = np.ones(distance_matrix.shape) / len(distance_matrix)\n",
    "        self.all_inds = range(len(distance_matrix))\n",
    "        self.n_ants = n_ants\n",
    "        self.n_best = n_best\n",
    "        self.n_iterations = n_iterations\n",
    "        self.decay = decay\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def run(self):\n",
    "        shortest_path = None\n",
    "        all_time_shortest_path = (\"placeholder\", np.inf)\n",
    "        for i in range(self.n_iterations):\n",
    "            all_paths = self.gen_all_paths()\n",
    "            self.spread_pheromone(all_paths, self.n_best)\n",
    "            shortest_path = min(all_paths, key=lambda x: x[1])\n",
    "            if shortest_path[1] < all_time_shortest_path[1]:\n",
    "                all_time_shortest_path = shortest_path\n",
    "            self.pheromone *= self.decay\n",
    "        return all_time_shortest_path\n",
    "\n",
    "    def spread_pheromone(self, all_paths, n_best):\n",
    "        sorted_paths = sorted(all_paths, key=lambda x: x[1])\n",
    "        for path, dist in sorted_paths[:n_best]:\n",
    "            for i in range(len(path) - 1):\n",
    "                move = (path[i], path[i + 1])\n",
    "                self.pheromone[move] += 1.0 / self.distance_matrix[move]\n",
    "\n",
    "    def gen_path_dist(self, path):\n",
    "        total_dist = 0\n",
    "        for i in range(len(path) - 1):\n",
    "            total_dist += self.distance_matrix[path[i], path[i+1]]\n",
    "        total_dist += self.distance_matrix[path[-1], path[0]]\n",
    "        return total_dist\n",
    "\n",
    "    def gen_all_paths(self):\n",
    "        all_paths = []\n",
    "        for i in range(self.n_ants):\n",
    "            path = self.gen_path(0)\n",
    "            all_paths.append((path, self.gen_path_dist(path)))\n",
    "        return all_paths\n",
    "\n",
    "    def gen_path(self, start):\n",
    "        path = [start]\n",
    "        visited = set(path)\n",
    "        prev = start\n",
    "        for i in range(len(self.distance_matrix) - 1):\n",
    "            move = self.pick_move(self.pheromone[prev], self.distance_matrix[prev], visited)\n",
    "            path.append(move)\n",
    "            visited.add(move)\n",
    "            prev = move\n",
    "        path.append(start)\n",
    "        return path\n",
    "\n",
    "    def pick_move(self, pheromone, dist, visited):\n",
    "        pheromone = np.copy(pheromone)\n",
    "        pheromone[list(visited)] = 0\n",
    "\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            heuristic = np.where(dist > 0, 1.0 / dist, 0)\n",
    "            row = pheromone * self.alpha * heuristic * self.beta\n",
    "\n",
    "        total = np.sum(row)\n",
    "        if total == 0 or not np.isfinite(total):\n",
    "            norm_row = np.ones(len(row)) / len(row)\n",
    "        else:\n",
    "            norm_row = row / total\n",
    "        \n",
    "        move = random.choices(self.all_inds, weights=norm_row, k=1)[0]\n",
    "        return move\n",
    "\n",
    "# --- Neural Network for Error Prediction ---\n",
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_shape=(input_shape,),\n",
    "                     kernel_regularizer=regularizers.l2(0.001)),  # L2 Regularization\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),  # Reduced dropout to retain more information\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # Added extra layer\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Adjusted learning rate\n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- XGBoost Model for Error Prediction ---\n",
    "def train_xgboost(x_train, y_train):\n",
    "    # Split the data into training and validation sets\n",
    "    x_train_part, x_val_part, y_train_part, y_val_part = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    dtrain = xgb.DMatrix(x_train_part, label=y_train_part)\n",
    "    dval = xgb.DMatrix(x_val_part, label=y_val_part)\n",
    "\n",
    "    params = {\n",
    "        'max_depth': 4,           # Adjust depth\n",
    "        'eta': 0.05,              # Learning rate\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'subsample': 0.8,         # Subsample to reduce overfitting\n",
    "        'colsample_bytree': 0.8,  # Feature sampling\n",
    "        'verbosity': 1\n",
    "    }\n",
    "\n",
    "    # Add validation data to the evals parameter\n",
    "    evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=500, evals=evals, early_stopping_rounds=20)\n",
    "    return model\n",
    "\n",
    "# --- Noise Reduction ---\n",
    "def apply_noise_reduction(signal, noise_level):\n",
    "    return signal / (1 + noise_level)\n",
    "\n",
    "# --- Function to Plot ROC Curve for Final XGBoost Model ---\n",
    "def plot_roc_curve_final(y_true, y_pred):\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve - Final XGBoost Model')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# --- Data Augmentation ---\n",
    "def augment_data(x_train, y_train):\n",
    "    noise_factor = 0.3\n",
    "    noisy_data = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "    noisy_data = np.clip(noisy_data, 0., 1.)\n",
    "    return np.concatenate((x_train, noisy_data)), np.concatenate((y_train, y_train))\n",
    "\n",
    "# --- Data Preprocessing and SMOTE ---\n",
    "def preprocess_data(x_train, y_train):\n",
    "    smote = SMOTE()\n",
    "    x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train_smote)\n",
    "    return x_train_scaled, y_train_smote\n",
    "\n",
    "# --- MARTAHO Simulation ---\n",
    "def martaho_simulation(distance_matrix, x_train, y_train, noise_level, interference_level, traffic_load):\n",
    "    # Solve initial TSP\n",
    "    tsp_route, _ = solve_tsp(distance_matrix)\n",
    "    \n",
    "    # Run ACO starting from TSP solution\n",
    "    ant_colony = AntColony(distance_matrix, n_ants=int(5*traffic_load), n_best=2, n_iterations=100, decay=0.95)\n",
    "    refined_route, _ = ant_colony.run()\n",
    "\n",
    "    # Data Preprocessing and Augmentation\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_train_augmented, y_train_augmented = augment_data(x_train_scaled, y_train)\n",
    "\n",
    "    # Train Neural Network for Error Prediction\n",
    "    model = build_model(x_train.shape[1])\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 10:\n",
    "            return lr\n",
    "        else:\n",
    "            return float(lr * tf.math.exp(-0.1).numpy())\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(x_train_augmented, y_train_augmented, epochs=100, batch_size=64, \n",
    "                        verbose=1, validation_split=0.2, callbacks=[lr_callback, early_stopping])\n",
    "\n",
    "    # Train XGBoost Model\n",
    "    x_train_processed, y_train_processed = preprocess_data(x_train, y_train)\n",
    "    xgb_model = train_xgboost(x_train_processed, y_train_processed)\n",
    "\n",
    "    # Extract training loss and accuracy\n",
    "    loss = history.history['loss']\n",
    "    accuracy = history.history['accuracy']\n",
    "    \n",
    "    # Predict probabilities for ROC Curve for XGBoost\n",
    "    dtrain = xgb.DMatrix(x_train_processed)\n",
    "    y_pred_xgb = xgb_model.predict(dtrain)\n",
    "\n",
    "    # Simulate signal transmission with noise reduction\n",
    "    signal = np.random.rand(100)\n",
    "    signal_with_interference = signal * (1 - interference_level)\n",
    "    cleaned_signal = apply_noise_reduction(signal_with_interference, noise_level)\n",
    "\n",
    "    # Output final route, processed signal, and training details\n",
    "    return refined_route, cleaned_signal, loss, accuracy, y_pred_xgb, y_train_processed\n",
    "\n",
    "# --- Simulation Setup ---\n",
    "noise_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "interference_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "traffic_loads = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "# Example data for neural network training (randomly generated for demonstration purposes)\n",
    "x_train = np.random.rand(1000, 10)\n",
    "y_train = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# Distance matrix for TSP (example data)\n",
    "distance_matrix = np.array([\n",
    "    [0, 2, 9, 10],\n",
    "    [1, 0, 6, 4],\n",
    "    [15, 7, 0, 8],\n",
    "    [6, 3, 12, 0]\n",
    "])\n",
    "\n",
    "# Run simulations and collect results\n",
    "results = []\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    for interference_level in interference_levels:\n",
    "        for traffic_load in traffic_loads:\n",
    "            print(f\"Testing with Noise Level: {noise_level}, Interference Level: {interference_level}, Traffic Load: {traffic_load}\")\n",
    "            refined_route, cleaned_signal, loss, accuracy, y_pred_xgb, y_train_processed = martaho_simulation(distance_matrix, x_train, y_train, noise_level, interference_level, traffic_load)\n",
    "            results.append({\n",
    "                'noise_level': noise_level,\n",
    "                'interference_level': interference_level,\n",
    "                'traffic_load': traffic_load,\n",
    "                'refined_route': refined_route,\n",
    "                'cleaned_signal_mean': np.mean(cleaned_signal),\n",
    "                'loss': loss,\n",
    "                'accuracy': accuracy,\n",
    "                'y_pred_xgb': y_pred_xgb,\n",
    "                'y_train_processed': y_train_processed\n",
    "            })\n",
    "\n",
    "# Plot ROC Curve for the final XGBoost Model from the last run\n",
    "print(\"Final XGBoost Model ROC Curve:\")\n",
    "plot_roc_curve_final(results[-1]['y_train_processed'], results[-1]['y_pred_xgb'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# --- TSP Component ---\n",
    "def solve_tsp(distance_matrix):\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
    "    return col_ind, distance_matrix[row_ind, col_ind].sum()\n",
    "\n",
    "# --- ACO Component ---\n",
    "class AntColony:\n",
    "    def __init__(self, distance_matrix, n_ants, n_best, n_iterations, decay, alpha=1, beta=2):\n",
    "        self.distance_matrix = distance_matrix\n",
    "        self.pheromone = np.ones(distance_matrix.shape) / len(distance_matrix)\n",
    "        self.all_inds = range(len(distance_matrix))\n",
    "        self.n_ants = n_ants\n",
    "        self.n_best = n_best\n",
    "        self.n_iterations = n_iterations\n",
    "        self.decay = decay\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def run(self):\n",
    "        shortest_path = None\n",
    "        all_time_shortest_path = (\"placeholder\", np.inf)\n",
    "        for i in range(self.n_iterations):\n",
    "            all_paths = self.gen_all_paths()\n",
    "            self.spread_pheromone(all_paths, self.n_best)\n",
    "            shortest_path = min(all_paths, key=lambda x: x[1])\n",
    "            if shortest_path[1] < all_time_shortest_path[1]:\n",
    "                all_time_shortest_path = shortest_path\n",
    "            self.pheromone *= self.decay\n",
    "        return all_time_shortest_path\n",
    "\n",
    "    def spread_pheromone(self, all_paths, n_best):\n",
    "        sorted_paths = sorted(all_paths, key=lambda x: x[1])\n",
    "        for path, dist in sorted_paths[:n_best]:\n",
    "            for i in range(len(path) - 1):\n",
    "                move = (path[i], path[i + 1])\n",
    "                self.pheromone[move] += 1.0 / self.distance_matrix[move]\n",
    "\n",
    "    def gen_path_dist(self, path):\n",
    "        total_dist = 0\n",
    "        for i in range(len(path) - 1):\n",
    "            total_dist += self.distance_matrix[path[i], path[i+1]]\n",
    "        total_dist += self.distance_matrix[path[-1], path[0]]\n",
    "        return total_dist\n",
    "\n",
    "    def gen_all_paths(self):\n",
    "        all_paths = []\n",
    "        for i in range(self.n_ants):\n",
    "            path = self.gen_path(0)\n",
    "            all_paths.append((path, self.gen_path_dist(path)))\n",
    "        return all_paths\n",
    "\n",
    "    def gen_path(self, start):\n",
    "        path = [start]\n",
    "        visited = set(path)\n",
    "        prev = start\n",
    "        for i in range(len(self.distance_matrix) - 1):\n",
    "            move = self.pick_move(self.pheromone[prev], self.distance_matrix[prev], visited)\n",
    "            path.append(move)\n",
    "            visited.add(move)\n",
    "            prev = move\n",
    "        path.append(start)\n",
    "        return path\n",
    "\n",
    "    def pick_move(self, pheromone, dist, visited):\n",
    "        pheromone = np.copy(pheromone)\n",
    "        pheromone[list(visited)] = 0\n",
    "\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            heuristic = np.where(dist > 0, 1.0 / dist, 0)\n",
    "            row = pheromone * self.alpha * heuristic * self.beta\n",
    "\n",
    "        total = np.sum(row)\n",
    "        if total == 0 or not np.isfinite(total):\n",
    "            norm_row = np.ones(len(row)) / len(row)\n",
    "        else:\n",
    "            norm_row = row / total\n",
    "\n",
    "        move = random.choices(self.all_inds, weights=norm_row, k=1)[0]\n",
    "        return move\n",
    "\n",
    "# --- Neural Network for Error Prediction ---\n",
    "def build_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_shape=(input_shape,),\n",
    "                     kernel_regularizer=regularizers.l2(0.001)),  # L2 Regularization\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),  # Reduced dropout to retain more information\n",
    "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # Added extra layer\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Adjusted learning rate\n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- XGBoost Model for Error Prediction ---\n",
    "def train_xgboost(x_train, y_train):\n",
    "    # Split the data into training and validation sets\n",
    "    x_train_part, x_val_part, y_train_part, y_val_part = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    dtrain = xgb.DMatrix(x_train_part, label=y_train_part)\n",
    "    dval = xgb.DMatrix(x_val_part, label=y_val_part)\n",
    "\n",
    "    params = {\n",
    "        'max_depth': 4,           # Adjust depth\n",
    "        'eta': 0.05,              # Learning rate\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'subsample': 0.8,         # Subsample to reduce overfitting\n",
    "        'colsample_bytree': 0.8,  # Feature sampling\n",
    "        'verbosity': 1\n",
    "    }\n",
    "\n",
    "    # Add validation data to the evals parameter\n",
    "    evals = [(dtrain, 'train'), (dval, 'validation')]\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=500, evals=evals, early_stopping_rounds=20)\n",
    "    return model\n",
    "\n",
    "# --- Plot ROC and Precision-Recall Curves ---\n",
    "def plot_precision_recall_roc(y_true, y_pred):\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return roc_auc\n",
    "\n",
    "# --- Noise Reduction ---\n",
    "def apply_noise_reduction(signal, noise_level):\n",
    "    return signal / (1 + noise_level)\n",
    "\n",
    "# --- Data Augmentation ---\n",
    "def augment_data(x_train, y_train):\n",
    "    noise_factor = 0.3\n",
    "    noisy_data = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "    noisy_data = np.clip(noisy_data, 0., 1.)\n",
    "    return np.concatenate((x_train, noisy_data)), np.concatenate((y_train, y_train))\n",
    "\n",
    "# --- Data Preprocessing and SMOTE ---\n",
    "def preprocess_data(x_train, y_train):\n",
    "    smote = SMOTE()\n",
    "    x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train_smote)\n",
    "    return x_train_scaled, y_train_smote\n",
    "\n",
    "# --- MARTAHO Simulation ---\n",
    "def martaho_simulation(distance_matrix, x_train, y_train, noise_level, interference_level, traffic_load):\n",
    "    # Solve initial TSP\n",
    "    tsp_route, _ = solve_tsp(distance_matrix)\n",
    "    \n",
    "    # Run ACO starting from TSP solution\n",
    "    ant_colony = AntColony(distance_matrix, n_ants=int(5*traffic_load), n_best=2, n_iterations=100, decay=0.95)\n",
    "    refined_route, _ = ant_colony.run()\n",
    "\n",
    "    # Data Preprocessing and Augmentation\n",
    "    scaler = StandardScaler()\n",
    "    x_train_scaled = scaler.fit_transform(x_train)\n",
    "    x_train_augmented, y_train_augmented = augment_data(x_train_scaled, y_train)\n",
    "\n",
    "    # Train Neural Network for Error Prediction\n",
    "    model = build_model(x_train.shape[1])\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: lr if epoch < 10 else float(lr * tf.math.exp(-0.1).numpy()))\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(x_train_augmented, y_train_augmented, epochs=100, batch_size=64, \n",
    "                        verbose=1, validation_split=0.2, callbacks=[lr_callback, early_stopping])\n",
    "\n",
    "    # Train XGBoost Model\n",
    "    x_train_processed, y_train_processed = preprocess_data(x_train, y_train)\n",
    "    xgb_model = train_xgboost(x_train_processed, y_train_processed)\n",
    "\n",
    "    # Extract training loss and accuracy\n",
    "    loss = history.history['loss']\n",
    "    accuracy = history.history['accuracy']\n",
    "    \n",
    "    # Predict probabilities for ROC Curve for XGBoost\n",
    "    dtrain = xgb.DMatrix(x_train_processed)\n",
    "    y_pred_xgb = xgb_model.predict(dtrain)\n",
    "\n",
    "    # Simulate signal transmission with noise reduction\n",
    "    signal = np.random.rand(100)\n",
    "    signal_with_interference = signal * (1 - interference_level)\n",
    "    cleaned_signal = apply_noise_reduction(signal_with_interference, noise_level)\n",
    "\n",
    "    # Output final route, processed signal, and training details\n",
    "    return refined_route, cleaned_signal, loss, accuracy, y_pred_xgb, y_train_processed, history\n",
    "\n",
    "# --- Simulation Setup ---\n",
    "noise_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "interference_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "traffic_loads = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "# Example data for neural network training (randomly generated for demonstration purposes)\n",
    "x_train = np.random.rand(1000, 10)\n",
    "y_train = np.random.randint(0, 2, 1000)\n",
    "\n",
    "# Distance matrix for TSP (example data)\n",
    "distance_matrix = np.array([\n",
    "    [0, 2, 9, 10],\n",
    "    [1, 0, 6, 4],\n",
    "    [15, 7, 0, 8],\n",
    "    [6, 3, 12, 0]\n",
    "])\n",
    "\n",
    "# Run simulations and collect results\n",
    "results = []\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    for interference_level in interference_levels:\n",
    "        for traffic_load in traffic_loads:\n",
    "            print(f\"Testing with Noise Level: {noise_level}, Interference Level: {interference_level}, Traffic Load: {traffic_load}\")\n",
    "            refined_route, cleaned_signal, loss, accuracy, y_pred_xgb, y_train_processed, history = martaho_simulation(distance_matrix, x_train, y_train, noise_level, interference_level, traffic_load)\n",
    "            results.append({\n",
    "                'noise_level': noise_level,\n",
    "                'interference_level': interference_level,\n",
    "                'traffic_load': traffic_load,\n",
    "                'refined_route': refined_route,\n",
    "                'cleaned_signal_mean': np.mean(cleaned_signal),\n",
    "                'loss': loss,\n",
    "                'accuracy': accuracy,\n",
    "                'y_pred_xgb': y_pred_xgb,\n",
    "                'y_train_processed': y_train_processed,\n",
    "                'history': history\n",
    "            })\n",
    "\n",
    "# Visualize results for MARTAHO simulation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(noise_levels, [result['cleaned_signal_mean'] for result in results[:len(noise_levels)]], 'o-', color='blue')\n",
    "plt.xlabel('Noise Level')\n",
    "plt.ylabel('Cleaned Signal Mean')\n",
    "plt.title('Cleaned Signal Mean vs Noise Level')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize loss and accuracy for last simulation\n",
    "history = results[-1]['history']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label=\"Training Accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve for the final XGBoost Model from the last run\n",
    "print(\"Final XGBoost Model ROC Curve:\")\n",
    "roc_auc = plot_precision_recall_roc(results[-1]['y_train_processed'], results[-1]['y_pred_xgb'])\n",
    "print(f'Final ROC AUC: {roc_auc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
